model_path = 'models/moss-moon-003-base'
output_dir =  "output_models/finetuned_moss_lora"
data_path = 'data/chinese_thesis'
per_device_train_batch_size =  4
gradient_accumulation_steps =  4
learning_rate = 1e-3
num_train_epochs =  5
lr_scheduler_type = "linear"
warmup_ratio =  0.1
logging_steps = 10
save_strategy = "steps"
save_steps = 500
optim = "adamw_torch"
fp16 = False
remove_unused_columns = False
ddp_find_unused_parameters = False
seed = 42
lora_rank = 4
lora_alpha = 32
lora_dropout = 0.05
compute_dtype='fp32'  # you can choose ['fp32', 'fp16', 'bf16']
max_len = 3000
eos_token_id=106028
pad_token_id=106028